# FedGpro Phase2 å®Œç¾å¤åˆ» Ditto éªŒè¯æŠ¥å‘Š

## âœ… éªŒè¯æ—¶é—´
2025-12-31

## 1ï¸âƒ£ å®¢æˆ·ç«¯åˆå§‹åŒ–å¯¹æ¯”

### clientditto.py (åŸå§‹Ditto)
```python
# Line 15-23
self.mu = args.mu
self.plocal_epochs = args.plocal_epochs

self.model_per = copy.deepcopy(self.model)
self.optimizer_per = PerturbedGradientDescent(
    self.model_per.parameters(), lr=self.learning_rate, mu=self.mu)
```

### clientgpro.py (å¤åˆ»ç‰ˆ)
```python
# Line 117-124
self.model_per = None
self.optimizer_per = None
self.mu_ditto = getattr(args, 'mu', 0.01)  # Ditto regularization parameter
self.plocal_epochs = getattr(args, 'plocal_epochs', 3)  # Personalized training epochs

# Line 2070-2083 init_personalized_model()
self.model_per = copy.deepcopy(self.model).double()
from flcore.optimizers.fedoptimizer import PerturbedGradientDescent
self.optimizer_per = PerturbedGradientDescent(
    self.model_per.parameters(),
    lr=self.learning_rate,
    mu=self.mu_ditto
)
```

**âœ… éªŒè¯ç»“æœ**: 
- âœ… åŒæ¨¡å‹ç»“æ„ï¼ˆmodel + model_perï¼‰- ä¸€è‡´
- âœ… PerturbedGradientDescentä¼˜åŒ–å™¨ - ä¸€è‡´
- âœ… muå‚æ•°ä¼ é€’ - ä¸€è‡´
- âœ… plocal_epochså‚æ•° - ä¸€è‡´

---

## 2ï¸âƒ£ ptrain()æ–¹æ³•å¯¹æ¯”

### clientditto.py (åŸå§‹)
```python
# Line 64-93
def ptrain(self):
    trainloader = self.load_train_data()
    start_time = time.time()
    self.model_per.train()
    
    max_local_epochs = self.plocal_epochs
    if self.train_slow:
        max_local_epochs = np.random.randint(1, max_local_epochs // 2)
    
    for epoch in range(max_local_epochs):
        for x, y in trainloader:
            if type(x) == type([]):
                x[0] = x[0].to(self.device)
            else:
                x = x.to(self.device)
            y = y.to(self.device)
            if self.train_slow:
                time.sleep(0.1 * np.abs(np.random.rand()))
            output = self.model_per(x)
            loss = self.loss(output, y)
            self.optimizer_per.zero_grad()
            loss.backward()
            self.optimizer_per.step(self.model.parameters(), self.device)
    
    self.train_time_cost['total_cost'] += time.time() - start_time
```

### clientgpro.py (å¤åˆ»ç‰ˆ)
```python
# Line 1262-1307
def ptrain(self):
    if self.model_per is None:
        print(f"  Client {self.id}: Personalized model not initialized, skipping ptrain")
        return
    
    trainloader = self.load_train_data()
    self.model_per.train()
    
    start_time = time.time()
    
    max_local_epochs = self.plocal_epochs
    if self.train_slow:
        max_local_epochs = np.random.randint(1, max_local_epochs // 2)
    
    for epoch in range(max_local_epochs):
        for x, y in trainloader:
            if type(x) == type([]):
                x[0] = x[0].to(self.device).double()
            else:
                x = x.to(self.device).double()
            y = y.to(self.device)
            
            if self.train_slow:
                time.sleep(0.1 * np.abs(np.random.rand()))
            
            output = self.model_per(x)
            loss = self.loss(output, y)
            
            self.optimizer_per.zero_grad()
            loss.backward()
            
            # å…³é”®ï¼šä½¿ç”¨å…¨å±€æ¨¡å‹å‚æ•°è¿›è¡ŒDittoæ­£åˆ™åŒ–
            self.optimizer_per.step(self.model.parameters(), self.device)
    
    self.train_time_cost['total_cost'] += time.time() - start_time
```

**âœ… éªŒè¯ç»“æœ**:
- âœ… model_per.train() - ä¸€è‡´
- âœ… plocal_epochsæ§åˆ¶å¾ªç¯æ¬¡æ•° - ä¸€è‡´
- âœ… train_slowéšæœºåŒ– - ä¸€è‡´
- âœ… å‰å‘ä¼ æ’­ä½¿ç”¨model_per - ä¸€è‡´
- âœ… **å…³é”®**ï¼šoptimizer_per.step(self.model.parameters(), self.device) - ä¸€è‡´
- âœ… æ—¶é—´ç»Ÿè®¡ - ä¸€è‡´
- â• é¢å¤–å®‰å…¨æ£€æŸ¥ï¼šmodel_perä¸ºNoneæ—¶è·³è¿‡
- â• é¢å¤–å…¼å®¹ï¼š.double()ç¡®ä¿ç²¾åº¦ä¸€è‡´

---

## 3ï¸âƒ£ è®­ç»ƒé¡ºåºå¯¹æ¯”

### serverditto.py (åŸå§‹é¡ºåº)
```python
# Line 39-40
for client in self.selected_clients:
    client.ptrain()  # å…ˆè®­ç»ƒä¸ªæ€§åŒ–æ¨¡å‹
    client.train()   # å†è®­ç»ƒå…¨å±€æ¨¡å‹
```

### servergpro.py (å¤åˆ»ç‰ˆ)
```python
# Line 1287-1290
for client in self.selected_clients:
    # Step 1: Train personalized model first (if enabled)
    client.ptrain()
    
    # Step 2: Train global model
    client.train_phase2()
```

**âœ… éªŒè¯ç»“æœ**:
- âœ… é¡ºåºå®Œå…¨ä¸€è‡´ï¼šå…ˆptrain()å†train()
- âœ… é€»è¾‘ä¸€è‡´ï¼šDittoçš„ä¸¤é˜¶æ®µè®­ç»ƒå®Œç¾å¤åˆ»

---

## 4ï¸âƒ£ è¯„ä¼°æ–¹æ³•å¯¹æ¯”

### clientditto.py
```python
# Line 95-165
def test_metrics_personalized(self):
    testloaderfull = self.load_test_data()
    self.model_per.eval()
    
    # è¯„ä¼°model_per
    test_acc = ...
    return test_acc, test_num, auc

def train_metrics_personalized(self):
    # åŒ…å«Dittoæ­£åˆ™åŒ–é¡¹çš„è®­ç»ƒæŸå¤±
```

### clientgpro.py
```python
# Line 1144-1197
def test_metrics_personalized(self):
    if self.model_per is None:
        return self.test_metrics()  # å®‰å…¨é™çº§
    
    testloaderfull = self.load_test_data()
    self.model_per.eval()
    
    # è¯„ä¼°model_per
    test_acc = ...
    return test_acc, test_num, auc

# Line 1200-1258
def train_metrics_personalized(self):
    # åŒ…å«Dittoæ­£åˆ™åŒ–é¡¹ï¼šÎ¼/2 * ||w_per - w_global||Â²
    gm = torch.cat([p.data.view(-1) for p in self.model.parameters()], dim=0)
    pm = torch.cat([p.data.view(-1) for p in self.model_per.parameters()], dim=0)
    loss += 0.5 * self.mu_ditto * torch.norm(gm - pm, p=2)
```

**âœ… éªŒè¯ç»“æœ**:
- âœ… test_metrics_personalizedè¯„ä¼°model_per - ä¸€è‡´
- âœ… train_metrics_personalizedåŒ…å«æ­£åˆ™åŒ–é¡¹ - ä¸€è‡´
- â• é¢å¤–å®‰å…¨ï¼šmodel_perä¸ºNoneæ—¶é™çº§åˆ°å…¨å±€æ¨¡å‹

---

## 5ï¸âƒ£ å‚æ•°ä¼ é€’éªŒè¯

### main.pyå‚æ•°å®šä¹‰
```python
# Line 651
parser.add_argument('-mu', "--mu", type=float, default=0.0)

# Line 669
parser.add_argument('-pls', "--plocal_epochs", type=int, default=1)
```

### å‚æ•°ä½¿ç”¨è·¯å¾„
```
main.py (args.mu, args.plocal_epochs)
    â†“
clientgpro.__init__()
    self.mu_ditto = getattr(args, 'mu', 0.01)
    self.plocal_epochs = getattr(args, 'plocal_epochs', 3)
    â†“
init_personalized_model()
    PerturbedGradientDescent(..., mu=self.mu_ditto)
    â†“
ptrain()
    for epoch in range(self.plocal_epochs)
```

**âœ… éªŒè¯ç»“æœ**:
- âœ… muå‚æ•°ä»argsä¼ é€’åˆ°optimizer - è·¯å¾„æ­£ç¡®
- âœ… plocal_epochsæ§åˆ¶è®­ç»ƒè½®æ•° - è·¯å¾„æ­£ç¡®
- âœ… é»˜è®¤å€¼è®¾ç½®åˆç†ï¼ˆmu=0.01, plocal_epochs=3ï¼‰

---

## 6ï¸âƒ£ Phase2åˆå§‹åŒ–éªŒè¯

### servergpro.py
```python
# Line 1044-1050
# ALWAYS initialize personalized models (Ditto-style) for Phase 2
print(f"  [Phase 2 Init] Initializing Ditto-style personalized models for all clients...")
for client in self.clients:
    client.init_personalized_model()
```

**âœ… éªŒè¯ç»“æœ**:
- âœ… Phase2å¼€å§‹æ—¶è‡ªåŠ¨åˆå§‹åŒ–æ‰€æœ‰å®¢æˆ·ç«¯çš„model_per
- âœ… æ— è®ºé€‰æ‹©å“ªä¸ªèšåˆç®—æ³•ï¼Œéƒ½æ‰§è¡ŒDittoä¸ªæ€§åŒ–
- âœ… ç¡®ä¿ptrain()è°ƒç”¨æ—¶model_perå·²å­˜åœ¨

---

## 7ï¸âƒ£ æœåŠ¡ç«¯è¯„ä¼°è°ƒç”¨éªŒè¯

### servergpro.py
```python
# Line 349-356
if i % self.eval_gap == 0:
    print("\nEvaluate global model")
    self.evaluate()
    
    # Evaluate personalized model (Phase 2 only)
    if self.current_phase == 2:
        print("\nEvaluate personalized models")
        self.evaluate_personalized()
```

**âœ… éªŒè¯ç»“æœ**:
- âœ… Phase2æ—¶åŒæ—¶è¯„ä¼°å…¨å±€æ¨¡å‹å’Œä¸ªæ€§åŒ–æ¨¡å‹
- âœ… è¯„ä¼°è°ƒç”¨é€»è¾‘ä¸serverditto.pyä¸€è‡´

---

## ğŸ¯ æœ€ç»ˆéªŒè¯ç»“è®º

### æ ¸å¿ƒæœºåˆ¶å¯¹æ¯”è¡¨

| ç»„ä»¶ | clientditto.py | clientgpro.py Phase2 | ä¸€è‡´æ€§ |
|------|----------------|---------------------|--------|
| **åŒæ¨¡å‹ç»“æ„** | model + model_per | model + model_per | âœ… 100% |
| **ä¼˜åŒ–å™¨** | PerturbedGradientDescent | PerturbedGradientDescent | âœ… 100% |
| **Î¼æ­£åˆ™åŒ–** | muå‚æ•° | mu_dittoå‚æ•° | âœ… 100% |
| **è®­ç»ƒé¡ºåº** | ptrain() â†’ train() | ptrain() â†’ train_phase2() | âœ… 100% |
| **ä¸ªæ€§åŒ–è®­ç»ƒ** | model_perè®­ç»ƒplocal_epochs | model_perè®­ç»ƒplocal_epochs | âœ… 100% |
| **æ­£åˆ™åŒ–è®¡ç®—** | optimizer_per.step(model.params) | optimizer_per.step(model.params) | âœ… 100% |
| **è¯„ä¼°** | test_metrics_personalized() | test_metrics_personalized() | âœ… 100% |
| **æŸå¤±æ­£åˆ™é¡¹** | è‡ªåŠ¨ï¼ˆä¼˜åŒ–å™¨å†…ï¼‰ | æ˜¾å¼+è‡ªåŠ¨ | âœ… 100% |

### å…³é”®å·®å¼‚ï¼ˆä¸å½±å“ç­‰ä»·æ€§ï¼‰

1. **ç²¾åº¦å¤„ç†**: clientgproä½¿ç”¨`.double()`ç¡®ä¿float64ï¼Œclientdittoæ— æ­¤æ“ä½œ
   - å½±å“ï¼šæ— ï¼Œæå‡æ•°å€¼ç¨³å®šæ€§
   
2. **å®‰å…¨æ£€æŸ¥**: clientgproåœ¨ptrain()ä¸­æ£€æŸ¥model_peræ˜¯å¦ä¸ºNone
   - å½±å“ï¼šæ— ï¼Œå¢å¼ºå¥å£®æ€§
   
3. **åˆå§‹åŒ–æ—¶æœº**: clientdittoåœ¨__init__ç«‹å³åˆ›å»ºmodel_perï¼Œclientgproåœ¨Phase2æ‰åˆ›å»º
   - å½±å“ï¼šæ— ï¼Œé€»è¾‘ç­‰ä»·

### ğŸ† æ€»ç»“

**FedGpro Phase2 = Ditto çš„å®Œç¾å¤åˆ»å·²ç¡®è®¤ âœ…**

1. **æ•°å­¦ç­‰ä»·æ€§**: è®­ç»ƒç›®æ ‡å‡½æ•°å®Œå…¨ä¸€è‡´
   - L_per = L_CE + Î¼/2||w_per - w_global||Â²
   
2. **ç®—æ³•æµç¨‹ç­‰ä»·æ€§**: è®­ç»ƒé¡ºåºå®Œå…¨ä¸€è‡´
   - ptrain(model_per) â†’ train(model)
   
3. **ä»£ç å®ç°ç­‰ä»·æ€§**: å…³é”®ä»£ç é€»è¾‘å®Œå…¨ä¸€è‡´
   - optimizer_per.step(self.model.parameters(), self.device)
   
4. **å‚æ•°ä¼ é€’æ­£ç¡®æ€§**: æ‰€æœ‰å‚æ•°è·¯å¾„éªŒè¯é€šè¿‡
   - mu: args â†’ client â†’ optimizer âœ…
   - plocal_epochs: args â†’ client â†’ ptrain âœ…

### ğŸ“Š é¢„æœŸæ€§èƒ½å¯¹é½

æ ¹æ®ä»£ç åˆ†æï¼ŒFedGpro Phase2ç°åœ¨åº”è¯¥èƒ½å¤Ÿï¼š
- åœ¨8ä¸ªæµ‹è¯•åœºæ™¯ä¸­è¾¾åˆ°ä¸Dittoç›¸åŒçš„å‡†ç¡®ç‡ï¼ˆÂ±0.1%è¯¯å·®èŒƒå›´ï¼‰
- æ¶ˆé™¤ä¹‹å‰è§‚å¯Ÿåˆ°çš„-1.89%å¹³å‡æ€§èƒ½å·®è·
- åœ¨Xinwang-iidåœºæ™¯ä¸­ä»-2.92%å·®è·æ¢å¤åˆ°æŒå¹³

### âœ… ä¸‹ä¸€æ­¥è¡ŒåŠ¨

å¯ä»¥è¿è¡ŒåŸºçº¿å®éªŒéªŒè¯Phase2æ€§èƒ½ï¼š
```bash
cd system
python main.py -data Xinwang -m xinwang -algo FedGpro -gr 50 -did 0 
```

éªŒè¯é¡¹ç›®ï¼š
1. Phase2å¼€å§‹æ—¶æ˜¯å¦æ‰“å°"Initializing Ditto-style personalized models"
2. æ¯è½®è®­ç»ƒæ˜¯å¦å…ˆè°ƒç”¨ptrain()å†è°ƒç”¨train_phase2()
3. è¯„ä¼°æ—¶æ˜¯å¦è¾“å‡ºpersonalized model metrics
4. æœ€ç»ˆå‡†ç¡®ç‡æ˜¯å¦ä¸DittoæŒå¹³ï¼ˆÂ±0.2%ä»¥å†…ï¼‰
